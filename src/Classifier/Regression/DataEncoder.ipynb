{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50cc21-a7f0-45c6-a62d-df3204260f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4d284-deb3-486f-ba29-2e54ec991911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c048b-6e42-4722-b7e3-bbdf9f63a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import re\n",
    "\n",
    "def parse_trajectories(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    blocks = content.strip().split(\"\\n\\n\")  # Split by blocks separated by blank lines\n",
    "    all_trajectories = []\n",
    "\n",
    "    for block in blocks:\n",
    "        lines = block.strip().split(\"\\n\")\n",
    "        trajectory = []\n",
    "\n",
    "        for line in lines:\n",
    "            actions = line.strip().split(\"-\")  # Each action is separated by '-'\n",
    "            timestep = []\n",
    "\n",
    "            for action in actions:\n",
    "                # Adjust the regex to capture multiple labels in the format pX(y1,y2,...)\n",
    "                match = re.match(r\"r(\\d+)\\((\\d+)\\):p(\\d+)\\(([^)]+)\\),p(\\d+)\\(([^)]+)\\)\", action.strip())\n",
    "                if match:\n",
    "                    r_id, duration, p_start, y_start_str, p_end, y_end_str = match.groups()\n",
    "                    \n",
    "                    # Extract numeric label values (e.g., 'y1', 'y7' → 1, 7)\n",
    "                    y_start = [int(label[1:]) for label in y_start_str.split(',')]\n",
    "                    y_end = [int(label[1:]) for label in y_end_str.split(',')]\n",
    "\n",
    "                    timestep.append({\n",
    "                        \"robot_id\": int(r_id),\n",
    "                        \"duration\": int(duration),\n",
    "                        \"p_start\": int(p_start),\n",
    "                        \"y_start\": y_start,\n",
    "                        \"p_end\": int(p_end),\n",
    "                        \"y_end\": y_end\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Warning: Could not parse action: {action}\")\n",
    "            trajectory.append(timestep)\n",
    "\n",
    "        all_trajectories.append(trajectory)\n",
    "\n",
    "    return all_trajectories\n",
    "\n",
    "# Parse the data from the log file\n",
    "parsed_data = parse_trajectories(\"log.txt\")\n",
    "\n",
    "# Print the parsed data for the first timestep in the first trajectory\n",
    "print(parsed_data[0][0])  # first timestep in that trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82cf9d-4697-4d41-b652-0b98d8467464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Compute max ids from the parsed data\n",
    "robot_ids = set()\n",
    "place_ids = set()\n",
    "\n",
    "for traj in parsed_data:\n",
    "    for timestep in traj:\n",
    "        for action in timestep:\n",
    "            robot_ids.add(action['robot_id'])\n",
    "            place_ids.add(action['p_start'])\n",
    "            place_ids.add(action['p_end'])\n",
    "\n",
    "max_robot_id = max(robot_ids) + 1\n",
    "max_place_id = max(place_ids) + 1\n",
    "max_timesteps = max(len(traj) for traj in parsed_data)\n",
    "\n",
    "# Then initialize your embeddings\n",
    "embed_dim = 32\n",
    "robot_embedding_matrix = nn.Embedding(max_robot_id, embed_dim)\n",
    "place_embedding_matrix = nn.Embedding(max_place_id, embed_dim)\n",
    "timestep_embedding_matrix = nn.Embedding(max_timesteps, embed_dim)\n",
    "\n",
    "# If you want to just use tensors here, extract their weight data:\n",
    "robot_embeddings = robot_embedding_matrix.weight.data  # shape (max_robot_id, embed_dim)\n",
    "place_embeddings = place_embedding_matrix.weight.data  # shape (max_place_id, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14969940-dbe4-418c-a2f2-585ce79d762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(value, dim=32):\n",
    "    \"\"\"Sinusoidal embedding for a scalar value.\"\"\"\n",
    "    pe = torch.zeros(dim)\n",
    "    position = torch.tensor(value, dtype=torch.float32)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2).float() * -(np.log(10000.0) / dim))\n",
    "    pe[0::2] = torch.sin(position * div_term)\n",
    "    pe[1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "def time_embedding(duration, max_duration):\n",
    "    \"\"\"Sinusoidal encoding for duration using 2D angle.\"\"\"\n",
    "    normalized = np.log(duration + 1) / np.log(max_duration + 1)\n",
    "    angle = 2 * np.pi * normalized\n",
    "    return torch.tensor([np.sin(angle), np.cos(angle)], dtype=torch.float32)\n",
    "\n",
    "def label_embedding(y_start, y_end, num_labels=5):\n",
    "    \"\"\"Label transition encoding: -1 for origin labels, 1 for destination labels, 0 otherwise.\"\"\"\n",
    "    vec = torch.zeros(num_labels)\n",
    "    for i in y_start:\n",
    "        vec[i - 1] = -1\n",
    "    for i in y_end:\n",
    "        if vec[i - 1] == -1:\n",
    "            vec[i - 1] = 0  # neutralize if label is both in start and end\n",
    "        else:\n",
    "            vec[i - 1] = 1\n",
    "    return vec\n",
    "\n",
    "def timestep_embedding(position, dim=32):\n",
    "    \"\"\"Sinusoidal positional encoding for timestep index.\"\"\"\n",
    "    pe = torch.zeros(dim)\n",
    "    position = torch.tensor(position, dtype=torch.float32)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2).float() * -(np.log(10000.0) / dim))\n",
    "    pe[0::2] = torch.sin(position * div_term)\n",
    "    pe[1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c97d3-c7a6-48c9-9755-f17b310d14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_actions(parsed_data, robot_embeddings, place_embeddings, timestep_embeddings, num_labels=5, embed_dim=32):\n",
    "    max_duration = max(\n",
    "        action['duration']\n",
    "        for trajectory in parsed_data\n",
    "        for timestep in trajectory\n",
    "        for action in timestep\n",
    "    )\n",
    "    \n",
    "    encoded_vectors = []\n",
    "\n",
    "    for trajectory in parsed_data:\n",
    "        encoded_trajectory = []\n",
    "\n",
    "        for t_idx, timestep in enumerate(trajectory):\n",
    "            timestep_vec = timestep_embeddings[t_idx]\n",
    "\n",
    "            encoded_timestep = []\n",
    "\n",
    "            for action in timestep:\n",
    "                # Instead of sinusoidal, do lookup here:\n",
    "                robot_vec = robot_embeddings[action['robot_id']]  # action['robot_id'] should be int index\n",
    "                p_start_vec = place_embeddings[action['p_start']]\n",
    "                p_end_vec = place_embeddings[action['p_end']]\n",
    "\n",
    "                label_vec = label_embedding(action['y_start'], action['y_end'], num_labels=num_labels)\n",
    "                time_vec = time_embedding(action['duration'], max_duration)\n",
    "\n",
    "                full_vec = torch.cat([\n",
    "                    robot_vec,\n",
    "                    p_start_vec,\n",
    "                    p_end_vec,\n",
    "                    label_vec,\n",
    "                    time_vec,\n",
    "                    timestep_vec\n",
    "                ])\n",
    "\n",
    "                encoded_timestep.append(full_vec)\n",
    "            encoded_trajectory.append(encoded_timestep)\n",
    "\n",
    "        encoded_vectors.append(encoded_trajectory)\n",
    "\n",
    "    return encoded_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec12eb-f401-430a-8261-c6bb7a1136eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data from the log file\n",
    "parsed_data = parse_trajectories(\"Datasets/Dataset1/BigDataset.txt\")\n",
    "\n",
    "# Set the number of labels manually\n",
    "num_labels = 7  # Change this based on the number of labels you want\n",
    "\n",
    "# Example: create embedding matrices (trainable parameters should be managed separately during training)\n",
    "robot_embedding_matrix = nn.Embedding(max_robot_id, embed_dim)\n",
    "place_embedding_matrix = nn.Embedding(max_place_id, embed_dim)\n",
    "\n",
    "# Pass the weight tensors:\n",
    "encoded_vectors = encode_actions(\n",
    "    parsed_data,\n",
    "    robot_embedding_matrix.weight.data,\n",
    "    place_embedding_matrix.weight.data,\n",
    "    timestep_embeddings=timestep_embedding_matrix.weight.data,\n",
    "    num_labels=num_labels,\n",
    "    embed_dim=embed_dim\n",
    ")\n",
    "\n",
    "# Print the encoded vector for the first timestep in the first trajectory\n",
    "print(encoded_vectors[0][0][0])  # First timestep in the first trajectory\n",
    "print(f\"Vector size: {encoded_vectors[0][0][0].shape}\")  # Expected size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b2214-524e-41c7-a62e-477e5b1795d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_trajectories(encoded_vectors):\n",
    "    max_timesteps = max(len(traj) for traj in encoded_vectors)\n",
    "    max_actions = max(len(ts) for traj in encoded_vectors for ts in traj)\n",
    "    feature_dim = encoded_vectors[0][0][0].shape[-1]\n",
    "\n",
    "    padded_trajectories = []\n",
    "    action_masks = []\n",
    "    timestep_masks = []\n",
    "\n",
    "    for traj in encoded_vectors:\n",
    "        padded_traj = []\n",
    "        traj_action_mask = []\n",
    "        for ts in traj:\n",
    "            # Pad actions within this timestep\n",
    "            real_actions = len(ts)\n",
    "            padded_ts = ts + [torch.zeros(feature_dim)] * (max_actions - real_actions)\n",
    "            padded_ts_tensor = torch.stack(padded_ts)  # (A, D)\n",
    "            padded_traj.append(padded_ts_tensor)\n",
    "\n",
    "            # Action mask: 1 for real actions, 0 for padded\n",
    "            action_mask = [1] * real_actions + [0] * (max_actions - real_actions)\n",
    "            traj_action_mask.append(torch.tensor(action_mask, dtype=torch.bool))  # (A,)\n",
    "\n",
    "        # Pad timesteps\n",
    "        num_real_timesteps = len(traj)\n",
    "        empty_ts = torch.zeros(max_actions, feature_dim)\n",
    "        padded_traj += [empty_ts] * (max_timesteps - num_real_timesteps)\n",
    "        padded_traj_tensor = torch.stack(padded_traj)  # (T, A, D)\n",
    "        padded_trajectories.append(padded_traj_tensor)\n",
    "\n",
    "        # Pad action mask\n",
    "        empty_action_mask = torch.zeros(max_actions, dtype=torch.bool)\n",
    "        traj_action_mask += [empty_action_mask] * (max_timesteps - num_real_timesteps)\n",
    "        action_masks.append(torch.stack(traj_action_mask))  # (T, A)\n",
    "\n",
    "        # Timestep mask: 1 for real timesteps, 0 for padded\n",
    "        timestep_mask = [1] * num_real_timesteps + [0] * (max_timesteps - num_real_timesteps)\n",
    "        timestep_masks.append(torch.tensor(timestep_mask, dtype=torch.bool))  # (T,)\n",
    "\n",
    "    return (\n",
    "        torch.stack(padded_trajectories),  # (B, T, A, D)\n",
    "        torch.stack(action_masks),         # (B, T, A)\n",
    "        torch.stack(timestep_masks),       # (B, T)\n",
    "    )\n",
    "\n",
    "\n",
    "data_tensor, action_mask, timestep_mask = pad_trajectories(encoded_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94505e2a-fa0a-46cd-94c7-552b5280f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Tensor Shape:\", data_tensor.shape)          # (B, T, A, D)\n",
    "print(\"Action Mask Shape:\", action_mask.shape)          # (B, T, A)\n",
    "print(\"Timestep Mask Shape:\", timestep_mask.shape)      # (B, T)\n",
    "\n",
    "# --- Check a specific trajectory ---\n",
    "i = 0  # Check the first trajectory\n",
    "\n",
    "print(f\"\\nTrajectory {i}:\")\n",
    "print(\"Timestep lengths:\", [sum(am).item() for am in action_mask[i]])  # Number of real actions per timestep\n",
    "print(\"Timestep mask:\", timestep_mask[i].int().tolist())               # 1 if real timestep, 0 if padded\n",
    "\n",
    "# --- Visual check for one timestep ---\n",
    "t = 2  # First timestep\n",
    "print(f\"\\nTrajectory {i}, Timestep {t} — Action Mask:\", action_mask[i][t].int().tolist())\n",
    "print(f\"Data tensor (values) shape: {data_tensor[i][t].shape}\")\n",
    "print(\"First action vector (real or zero):\", data_tensor[i][t][0])  # Show first 5 dims of first action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da2c70-813f-4da4-a1a1-d23c0ce7465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Assuming you already have these:\n",
    "# data_tensor: shape (B, T, A, D)\n",
    "# action_mask: shape (B, T, A)\n",
    "# timestep_mask: shape (B, T)\n",
    "\n",
    "# 1. Create a TensorDataset\n",
    "dataset = TensorDataset(data_tensor, action_mask, timestep_mask)\n",
    "\n",
    "# 2. Define split sizes (e.g., 70% train, 15% val, 15% test)\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# 3. Random split\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# 4. Create DataLoaders\n",
    "batch_size = 16  # or any other suitable size\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57ad36-0688-4bf0-bfb7-0919aab69153",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Validation batches:\", len(val_loader))\n",
    "print(\"Test batches:\", len(test_loader))\n",
    "\n",
    "# Check one batch\n",
    "for data_batch, action_mask_batch, timestep_mask_batch in train_loader:\n",
    "    print(\"Data batch shape:\", data_batch.shape)  # (B, T, A, D)\n",
    "    print(\"Action mask shape:\", action_mask_batch.shape)  # (B, T, A)\n",
    "    print(\"Timestep mask shape:\", timestep_mask_batch.shape)  # (B, T)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
